# The main entry point of your workflow for smallRNA Pipeline
# After configuring, running snakemake -n in a clone of this repository should
# successfully execute a dry-run of the workflow.

import pandas as pd
from snakemake.utils import validate, min_version


##### set minimum snakemake version #####
min_version("7.8.3")


##### load config and sample sheets #####
configfile: "config/config.yaml"


validate(config, schema="schemas/config.schema.yaml")

samples = pd.read_table(config["samples"]).set_index("sample_name", drop=False)
validate(samples, schema="schemas/samples.schema.yaml")

units = pd.read_table(config["units"], dtype=str).set_index(
    ["sample_name", "unit_name"], drop=False
)
units.index = units.index.set_levels(
    [i.astype(str) for i in units.index.levels]
)  # enforce str in index
validate(units, schema="schemas/units.schema.yaml")

exogenous_sorted_bam_files = []
for unit in units.itertuples():
    sample_info = samples.loc[unit.sample_name]
    exogenous_sorted_bam_files.append(
        f"results/alignments/{sample_info.pegRNA}/sorted/{unit.sample_name}_{unit.unit_name}.bam",
    )


##### target rules #####
rule all:
    input:
        "results/qc/multiqc.html",  # from quality_control
        expand(
            "results/mapped_sorted/{sample}_{unit}.bam",
            zip,
            sample=units.sample_name,
            unit=units.unit_name,
        ),
        "data/references/grch38_pjy103_pjy300.fa.fai",


rule all_trimming:
    input:
        expand(
            "results/trimmed/{sample}_{unit}.{read}.fastq.gz",
            sample=units.sample_name,
            unit=units.unit_name,
            read=["1", "2"],
        ),


rule all_mapping:
    input:
        expand(
            "results/alignments/Homo_sapiens.GRCh38.dna.primary_assembly/sorted/{sample}_{unit}.bam",
            sample=units.sample_name,
            unit=units.unit_name,
        ),
        expand(
            "results/alignments/exogenous_rna/sorted/{sample}_{unit}.bam",
            sample=units.sample_name,
            unit=units.unit_name,
        ),


rule all_bedpe:
    input:
        expand(
            "results/pegrna_bedpe/{sample}_{unit}_pegrna.bedpe",
            sample=units.sample_name,
            unit=units.unit_name,
        ),


##### setup container #####


# this container defines the underlying OS for each job when using the workflow
# with --use-conda --use-singularity
# container: "docker://continuumio/miniconda3"


##### setup report #####


report: "report/workflow.rst"


##### load rules #####


include: "rules/common.smk"
include: "rules/genome.smk"
include: "rules/trimming.smk"
include: "rules/mapping.smk"
include: "rules/quality_control.smk"
include: "rules/alignment_reporting.smk"
